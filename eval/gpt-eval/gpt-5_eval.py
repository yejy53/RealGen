import os
import json
import base64
import io
from PIL import Image
from openai import OpenAI
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import random
import threading

# ==== 1. æ‰¹é‡é…ç½® (åœ¨è¿™é‡Œä¿®æ”¹) ====

# å­˜æ”¾æ‰€æœ‰JSONLè¾“å‡ºçš„åŸºç¡€ç›®å½•
BASE_OUTPUT_DIR = ""

# (æ–°) å¾…åˆ†æçš„æ–‡ä»¶å¤¹åˆ—è¡¨ (åŒ…æ‹¬çœŸå®å›¾ç‰‡å’ŒAIç”Ÿæˆçš„å›¾ç‰‡)
ALL_DIRS_TO_ANALYZE = [
    # ä½ çš„AIç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å¤¹
    
]

# ==== 2. API å’Œçº¿ç¨‹é…ç½® (ä¿æŒä¸å˜) ====
client = OpenAI(
    base_url="",
    api_key=""
)

write_lock = threading.Lock()
MAX_WORKERS = 200

# ==== 3. æ–°çš„ Prompt æ„å»ºå‡½æ•° (æ¥è‡ªä½ çš„ç¡®è®¤) ====
def build_prompt(image_path, base64_str):
    """
    æ„å»ºç”¨äºå›¾åƒçœŸå®æ€§æ£€æµ‹çš„Promptã€‚
    
    æœ€ç»ˆä¼˜åŒ–ç‰ˆï¼š
    1. ç¡®ä¿è¯„ä¼°çš„å®¢è§‚ã€ä¸¥æ ¼ã€å…¬æ­£ï¼Œæ¶ˆé™¤äº†è¯±å¯¼æ€§åè§ã€‚
    2. å¢åŠ äº†éäººåƒçš„æ£€æµ‹æ ‡å‡†ï¼ˆç‰©ç†ã€æ–‡å­—ã€çº¹ç†ï¼‰ã€‚
    3. ä¸“é—¨å¢åŠ äº†ä¸€æ¡é’ˆå¯¹äººåƒï¼ˆç‰¹åˆ«æ˜¯çš®è‚¤æ²¹è…»æ„Ÿï¼‰çš„æ£€æµ‹æ ‡å‡†ã€‚
    """
    # image_path åœ¨æ­¤å‡½æ•°ä¸­æœªè¢«ä½¿ç”¨ï¼Œä½†ä¿ç•™ç­¾åä»¥åŒ¹é…æ—§ç”¨æ³•
    return [
        {"role": "user", "content": """Assume you are an advanced **forensic image analyst** and AI detection expert. Your task is to conduct an **objective, rigorous, and strictly impartial evaluation** of the following image to determine if it is **real (photographic) or AI-generated (fake)**.

**You must apply a high standard of scrutiny to *all* images.** Do not default to 'Real' or 'Fake'; your conclusion must be based *only* on the evidence present in the image.

Consider the following criteria for your forensic judgment:

1.  **Physical & Geometric Logic:** Analyze physical interactions. Are shadows cast correctly according to light sources? Are reflections plausible and consistent? Do objects rest on surfaces naturally? Scrutinize architecture and rigid objects for warped lines, impossible geometry, or inconsistent perspectives.
2.  **Text, Symbols, and Fine Detail:** Examine any text, logos, or signs. AI-generated text is often nonsensical, misspelled, or has warped/merged characters. This is a very strong indicator.
3.  **General Texture & Surface Coherence:** Look at surfaces like wood, metal, water, or fabric. AI often produces textures that are overly smooth, blurry, lack fine natural detail (like wood grain), or have strange, repetitive patterns.
4.  **Human & Animal Subjects (if present):** If figures are present, scrutinize them for specific AI artifacts. Look for:
    * Distorted anatomical features (e.g., malformed hands, incorrect number of fingers, unnatural eyes/ears).
    * **Unnatural skin texture**, such as an **overly smooth, "greasy," or "plastic-like" sheen** that lacks natural pores and imperfections.
5.  **Lighting & Color Consistency:** Is the lighting consistent across the entire image? Do different elements look like they belong in the same environment? Look for unnatural color bleed or overly synthetic, "flat" saturation.
6.  **Edges & Artifacts:** Check the edges where objects meet. Look for unnatural sharpness, strange blurring (artifacting), or a 'cut-out' appearance that doesn't match a natural depth of field.

Please output your result in the *exact* following format:
Real or Fake: [Choose one]
Reason: [Provide a concise, objective explanation for your judgment, *directly referencing the specific criteria* (e.g., "Text is warped," "Skin texture appears overly plastic," "Shadows are physically inconsistent") that led to your conclusion.]
"""},
        {"role": "user", "content": [
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_str}"}}
        ]}
    ]

# ==== 4. å·¥å…·å‡½æ•° (å·²æ›´æ–°) ====
def encode_image_to_base64(image_path):
    """å°†å•ä¸ªå›¾åƒæ–‡ä»¶ç¼–ç ä¸ºBase64ã€‚"""
    with Image.open(image_path) as img:
        img = img.convert("RGB")
        buffer = io.BytesIO()
        img.save(buffer, format="JPEG")
        return base64.b64encode(buffer.getvalue()).decode("utf-8")

def load_jsonl_records(path: str):
    """
    åŠ è½½JSONLè®°å½•ä»¥è¿›è¡Œæ–­ç‚¹ç»­ä¼ ã€‚
    (å·²æ›´æ–°ï¼šç°åœ¨æ£€æŸ¥ 'image_name' è€Œä¸æ˜¯ 'image_pair')
    """
    records = []
    done = set()
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    rec = json.loads(line)
                    records.append(rec)
                    if "image_name" in rec:
                        done.add(rec["image_name"])
                except Exception:
                    continue
    return records, done

# ==== 5. æ ¸å¿ƒåˆ†æé€»è¾‘ (æ–°å‡½æ•°) ====
def process_single_image(image_path, max_retries=5, backoff_base=1.0):
    """
    å¤„ç†å•ä¸ªå›¾åƒå¹¶è¿”å›AIçš„ (Real/Fake) åˆ¤æ–­ç»“æœã€‚
    """
    fname = os.path.basename(image_path)
    
    try:
        base64_str = encode_image_to_base64(image_path)
        messages = build_prompt(image_path, base64_str)
    except Exception as e:
        return {"image_name": fname, "image_path": image_path, "response": None, "error": f"Image encode error: {e}"}

    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            response = client.chat.completions.create(
                model="gpt-5",
                messages=messages
            )
            content = response.choices[0].message.content
            response_text = (content or "").strip()

            if response_text:
                return {"image_name": fname, "image_path": image_path, "response": response_text}
            raise RuntimeError("empty response")

        except Exception as e:
            last_err = str(e)
            if attempt < max_retries:
                sleep_s = backoff_base * (2 ** (attempt - 1)) + random.random() * 0.5
                time.sleep(sleep_s)
                continue
            return {"image_name": fname, "image_path": image_path, "response": None, "error": last_err or "Max retries exceeded"}

    return {"image_name": fname, "image_path": image_path, "response": None, "error": last_err or "Unknown error"}

def run_analysis_on_directory(image_dir, output_jsonl):
    """
    è¿è¡Œä¸€æ¬¡å®Œæ•´çš„åˆ†ææµç¨‹ã€‚
    Args:
        image_dir (str): å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„ã€‚
        output_jsonl (str): æ­¤æ¬¡åˆ†æçš„ç»“æœ .jsonl æ–‡ä»¶è·¯å¾„ã€‚
    """
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_jsonl), exist_ok=True)
    
    records, done_images = load_jsonl_records(output_jsonl)

    # 1. æŸ¥æ‰¾æ–‡ä»¶
    try:
        all_files = {f for f in os.listdir(image_dir) if f.lower().endswith((".jpg", ".jpeg", ".png"))}
    except FileNotFoundError as e:
        print(f"\n[Error] æ— æ³•æ‰¾åˆ°æ–‡ä»¶å¤¹: {e.filename}ã€‚è·³è¿‡æ­¤æ¬¡åˆ†æã€‚")
        return

    all_paths = sorted([os.path.join(image_dir, f) for f in all_files])
    to_process = [p for p in all_paths if os.path.basename(p) not in done_images]
    
    print(f" Â > æ‰¾åˆ° {len(all_paths)} ä¸ªå›¾åƒã€‚")
    print(f" Â > âœ… å·²å®Œæˆ: {len(done_images)} | ğŸš€ å¾…å¤„ç†: {len(to_process)}")

    # (æ–°) ç»Ÿè®¡æ ‡å‡†
    counts = {"Real": 0, "Fake": 0, "Unknown/Error": 0}
    
    # (æ–°) ä»å·²æœ‰çš„è®°å½•ä¸­æ¢å¤è®¡æ•°
    for rec in records:
        response_lower = rec.get("response", "").lower()
        if "real or fake: real" in response_lower:
            counts["Real"] += 1
        elif "real or fake: fake" in response_lower:
            counts["Fake"] += 1
        else:
            counts["Unknown/Error"] += 1

    if not to_process:
        print(" Â > æ— éœ€å¤„ç†ï¼Œè·³è‡³ä¸‹ä¸€ä¸ªã€‚")
        # å³ä½¿æ— éœ€å¤„ç†ï¼Œä¹Ÿæ‰“å°å·²æœ‰çš„ç»Ÿè®¡ç»“æœ
        print(f"\n Â --- ğŸ“Š åˆ†ææ€»ç»“ ({os.path.basename(image_dir)}) ---")
        print(f" Â åˆ¤æ–­ä¸º Real: {counts['Real']} æ¬¡")
        print(f" Â åˆ¤æ–­ä¸º Fake: {counts['Fake']} æ¬¡")
        print(f" Â æ— æ³•åˆ¤æ–­ / å‡ºç°é”™è¯¯: {counts['Unknown/Error']} æ¬¡")
        return

    # 2. å¤šçº¿ç¨‹å¤„ç†
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor, open(output_jsonl, "a", encoding="utf-8") as f:
        futures = {executor.submit(process_single_image, img_path): img_path for img_path in to_process}

        for future in tqdm(as_completed(futures), total=len(futures), desc=f"Analyzing {os.path.basename(image_dir)}"):
            img_path = futures[future]
            try:
                result = future.result()
                response_text = result.get("response")

                if response_text:
                    with write_lock:
                        f.write(json.dumps(result, ensure_ascii=False) + "\n")
                        f.flush()
                        os.fsync(f.fileno())

                    response_lower = response_text.lower()
                    if "real or fake: real" in response_lower:
                        counts["Real"] += 1
                    elif "real or fake: fake" in response_lower:
                        counts["Fake"] += 1
                    else:
                        counts["Unknown/Error"] += 1
                else:
                    counts["Unknown/Error"] += 1
                    print(f"\n[Warning] å›¾åƒ {os.path.basename(img_path)} å¤„ç†å‡ºé”™: {result.get('error')}")

            except Exception as e:
                counts["Unknown/Error"] += 1
                print(f"\n[Critical] ä»»åŠ¡ {os.path.basename(img_path)} å¤±è´¥: {e}")

    print(f"\n Â > âœ… å…¨éƒ¨å®Œæˆã€‚ åˆ†æç»“æœå·²ä¿å­˜åˆ° -> {output_jsonl}")

    # 3. æ‰“å°æ€»ç»“
    print(f"\n Â --- ğŸ“Š åˆ†ææ€»ç»“ ({os.path.basename(image_dir)}) ---")
    print(f" Â åˆ¤æ–­ä¸º Real: {counts['Real']} æ¬¡")
    print(f" Â åˆ¤æ–­ä¸º Fake: {counts['Fake']} æ¬¡")
    print(f" Â æ— æ³•åˆ¤æ–­ / å‡ºç°é”™è¯¯: {counts['Unknown/Error']} æ¬¡")


# ==== 6. ä¸»æ§åˆ¶å™¨ (å·²ä¿®å¤) ====
def main():
    """
    ä¸»æ§åˆ¶å™¨ï¼Œå¾ªç¯æ‰§è¡Œæ‰€æœ‰åˆ†æä»»åŠ¡ã€‚
    """
    print("===== ğŸ¤– å¼€å§‹æ‰¹é‡å›¾åƒåˆ†æä»»åŠ¡ =====")
    print(f"è¾“å‡ºç›®å½•: {BASE_OUTPUT_DIR}")
    print(f"æ€»ä»»åŠ¡æ•°: {len(ALL_DIRS_TO_ANALYZE)}")
    print("---------------------------------")

    for i, image_dir in enumerate(ALL_DIRS_TO_ANALYZE):
        
        # --- (BUG ä¿®å¤å¼€å§‹) ---
        try:
            # ç§»é™¤è·¯å¾„æœ«å°¾å¯èƒ½å­˜åœ¨çš„æ–œæ  (e.g., /.../gpt/photo/ -> /.../gpt/photo)
            clean_path = image_dir.rstrip(os.path.sep)
            
            # è·å–è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†
            last_part = os.path.basename(clean_path)
            
            if last_part.lower() == "photo":
                # å¦‚æœæœ€åæ˜¯ 'photo', æˆ‘ä»¬å–å®ƒä¸Šä¸€çº§çš„ç›®å½•å
                # e.g., /.../gpt/photo -> "gpt"
                model_name = os.path.basename(os.path.dirname(clean_path))
            else:
                # å¦åˆ™ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨æœ€åä¸€éƒ¨åˆ†
                # e.g., /.../real-img-banchmark -> "real-img-banchmark"
                model_name = last_part
        
        except Exception:
            # å¤‡ç”¨æ–¹æ¡ˆï¼Œä»¥é˜²è·¯å¾„éå¸¸å¥‡æ€ª
            model_name = f"task_{i}"
        # --- (BUG ä¿®å¤ç»“æŸ) ---
            
        # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_file_path = os.path.join(BASE_OUTPUT_DIR, f"analysis_{model_name}.jsonl")

        print(f"\n===== ğŸš€ [ä»»åŠ¡ {i+1}/{len(ALL_DIRS_TO_ANALYZE)}] å¼€å§‹åˆ†æ =====")
        print(f"  åˆ†æç›®å½•: {model_name}")
        print(f"  å®Œæ•´è·¯å¾„: {image_dir}")
        print(f"  è¾“å‡ºæ–‡ä»¶:   {output_file_path}")
        print("---------------------------------")

        # æ‰§è¡Œå•æ¬¡åˆ†æ
        run_analysis_on_directory(
            image_dir=image_dir,
            output_jsonl=output_file_path
        )

        print(f"===== âœ… [ä»»åŠ¡ {i+1}/{len(ALL_DIRS_TO_ANALYZE)}] {model_name} åˆ†æå®Œæˆ =====")

    print("\n===== ğŸ‰ æ‰€æœ‰æ‰¹é‡ä»»åŠ¡å‡å·²å®Œæˆ! =====")

if __name__ == "__main__":
    main()